{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5975,
     "status": "ok",
     "timestamp": 1668444681644,
     "user": {
      "displayName": "AKASH SINGH IIIT Dharwad",
      "userId": "09449469462329399013"
     },
     "user_tz": -330
    },
    "id": "11y_109IaNyM"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This script is used to train the model using attention on hmm/pssm features along with bigram features (extracted using a CNN model).\n",
    "'''\n",
    "\n",
    "# import libraries\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Layer, Dense, Dropout, LSTM, GRU, Conv1D, Conv2D, Conv2DTranspose, MaxPooling2D, AveragePooling2D, UpSampling2D\n",
    "from tensorflow.keras.layers import concatenate, GlobalMaxPooling1D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Activation, Reshape, TimeDistributed, Embedding, Input\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adamax, Adadelta, Adagrad, Nadam\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "import parse_files as p\n",
    "from features import bigram_features0, bigram_features1, bigram_features2, bigram_features3, bigram_features4, bigram_features5\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsfeature = False # True/False (True if you want to use class features. Use only when predtype is Fold)\n",
    "rawdata = 'hmm' # hmm/pssm\n",
    "predtype = 'Fold' # Class/Fold\n",
    "dataset = 'SCOPe' # dd/edd/tg/SCOPe/25_SCOPe_DDEDDTG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 420\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# initialise empty lists for storing data\n",
    "labels = []\n",
    "hmm = []\n",
    "pssm = []\n",
    "seq = []\n",
    "seqlen = 200\n",
    "biGram_features0 = []\n",
    "biGram_features1 = []\n",
    "biGram_features2 = []\n",
    "biGram_features3 = []\n",
    "biGram_features4 = []\n",
    "biGram_features5 = []\n",
    "\n",
    "\n",
    "# load all the filenames of PSSM's\n",
    "filelist = glob.glob('./data/'+dataset+'/'+rawdata+'/*.txt')\n",
    "\n",
    "# read all the labels of the given dataset\n",
    "if dataset == \"SCOPe\":\n",
    "\tlabel_for_seq = pd.read_csv(\"./astral_2_08_final.csv\") # make sure all the sequences are in uppercase\n",
    "else:\n",
    "\tlabel_for_seq = p.load_labels('./data/'+dataset+'_'+predtype+'_labels.txt')\n",
    "\n",
    "# read all the HMM and PSSM matrices of the given dataset\n",
    "for i in range(0, len(filelist)):\n",
    "\t# HMM data\n",
    "\tif(rawdata == 'hmm'):\n",
    "\t\tseq_hmm,prob_hmm,extras_hmm = p.parse_hmm(filelist[i]) # parse hmm data\n",
    "\t\ttempseq = seq_hmm.upper() # convert the sequence to uppercase\n",
    "\t\tseq.append(tempseq) # append the sequence to the list\n",
    "\t\t# get the label\n",
    "\t\tif dataset == \"SCOPe\": \n",
    "\t\t\tlabels.append(label_for_seq.loc[label_for_seq[\"sequence\"] == tempseq][\"fold\"].values[0])\n",
    "\t\telse: \n",
    "\t\t\tlabels.append(label_for_seq[seq_hmm.upper()])\n",
    "\t\tif(clsfeature): # if use class features, append them (use only with Fold prediction)\n",
    "\t\t\tbiGram_features0.append(((np.append((bigram_features0(prob_hmm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\t\tbiGram_features1.append(((np.append((bigram_features1(prob_hmm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\t\tbiGram_features2.append(((np.append((bigram_features2(prob_hmm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\t\tbiGram_features3.append(((np.append((bigram_features3(prob_hmm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\t\tbiGram_features4.append(((np.append((bigram_features4(prob_hmm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\t\tbiGram_features5.append(((np.append((bigram_features5(prob_hmm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\telse: # use only bigram features\n",
    "\t\t\tbiGram_features0.append(bigram_features0(prob_hmm))\n",
    "\t\t\tbiGram_features2.append(bigram_features2(prob_hmm))\n",
    "\t\t\tbiGram_features3.append(bigram_features3(prob_hmm))\n",
    "\t\t\tbiGram_features4.append(bigram_features4(prob_hmm))\n",
    "\t\t\tbiGram_features1.append(bigram_features1(prob_hmm))\n",
    "\t\t\tbiGram_features5.append(bigram_features5(prob_hmm))\n",
    "\n",
    "\t\tnorm_hmm = prob_hmm + 0.01\n",
    "\t\tif(len(norm_hmm) < seqlen): # pad the sequence with zeros if it is less than the required length\n",
    "\t\t\tfor j in range(seqlen-len(norm_hmm)):\n",
    "\t\t\t\tnorm_hmm = np.concatenate((norm_hmm,norm_hmm[0]*0))\n",
    "\t\telse:\n",
    "\t\t\tnorm_hmm = norm_hmm[:seqlen] # truncate the sequence if it is more than the required length\n",
    "\t\thmm.append(norm_hmm)\n",
    "\n",
    "\t# PSSM data\n",
    "\telse:  \n",
    "\t\tseq_pssm,prob_pssm,lprob_pssm,extra_pssm = p.parse_pssm(filelist[i]) # get the pssm data\n",
    "\t\ttempseq = seq_pssm.upper() # convert the sequence to uppercase\n",
    "\t\tseq.append(tempseq) # append the sequence to the list\n",
    "\t\tlabels.append(label_for_seq[seq_pssm.upper()]) # get the label\n",
    "\t\tif(clsfeature): # if use class features, append them (use only with Fold prediction)\n",
    "\t\t\tbiGram_features0.append(((np.append((bigram_features0(prob_pssm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\t\tbiGram_features1.append(((np.append((bigram_features1(prob_pssm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\t\tbiGram_features2.append(((np.append((bigram_features2(prob_pssm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\t\tbiGram_features3.append(((np.append((bigram_features3(prob_pssm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\t\tbiGram_features4.append(((np.append((bigram_features4(prob_pssm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\t\tbiGram_features5.append(((np.append((bigram_features5(prob_pssm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\telse: # use only bigram features\n",
    "\t\t\tbiGram_features0.append(bigram_features0(prob_pssm))\n",
    "\t\t\tbiGram_features2.append(bigram_features2(prob_pssm))\n",
    "\t\t\tbiGram_features3.append(bigram_features3(prob_pssm))\n",
    "\t\t\tbiGram_features4.append(bigram_features4(prob_pssm))\n",
    "\t\t\tbiGram_features1.append(bigram_features1(prob_pssm))\n",
    "\t\t\tbiGram_features5.append(bigram_features5(prob_pssm))\n",
    "\n",
    "\t\tnorm_pssm = prob_pssm + 0.01\n",
    "\n",
    "\t\tif(len(norm_pssm) < seqlen): # pad the sequence with zeros if it is less than the required length\n",
    "\t\t\tfor j in range(seqlen-len(norm_pssm)):\n",
    "\t\t\t\tnorm_pssm = np.concatenate((norm_pssm,norm_pssm[0]*0))\n",
    "\t\telse: # truncate the sequence if it is more than the required length\n",
    "\t\t\tnorm_pssm = norm_pssm[:seqlen]\n",
    "\t\tpssm.append(norm_pssm)\n",
    "\n",
    "# convert everything to numpy arrays\n",
    "labels = np.array(labels)\n",
    "# print(\"Labels=\",labels)\n",
    "num_classes =  len(np.unique(labels))\n",
    "foldlabels = pd.get_dummies(labels).values\n",
    "# print(\"foldlabels=\",foldlabels)\n",
    "sequences = np.array(seq)\n",
    "biGram0 = np.array(biGram_features0)\n",
    "biGram1 = np.array(biGram_features1)\n",
    "biGram2 = np.array(biGram_features2)\n",
    "biGram3 = np.array(biGram_features3)\n",
    "biGram4 = np.array(biGram_features4)\n",
    "biGram5 = np.array(biGram_features5)\n",
    "hmm = np.array(hmm)\n",
    "pssm = np.array(pssm)\n",
    "\n",
    "if(rawdata == 'hmm'): # use hmm data\n",
    "\tmatrixdata = hmm\n",
    "else: # use pssm data\n",
    "\tmatrixdata = pssm\n",
    "\n",
    "no_filters1 = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLUYxzRZbVKA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_6728\\1542452974.py:6: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n",
      "Fold- 1 :  [1.637765645980835, 0.8464000225067139]\n",
      "20/20 [==============================] - 1s 11ms/step\n",
      "pred_scores shape = (625, 1, 171)\n",
      "Fold- 2 :  [1.6080728769302368, 0.8464000225067139]\n",
      "20/20 [==============================] - 0s 11ms/step\n",
      "pred_scores shape = (625, 1, 171)\n",
      "Fold- 3 :  [1.7202656269073486, 0.8399999737739563]\n",
      "20/20 [==============================] - 0s 11ms/step\n",
      "pred_scores shape = (625, 1, 171)\n",
      "Fold- 4 :  [1.7978019714355469, 0.8159999847412109]\n",
      "20/20 [==============================] - 0s 11ms/step\n",
      "pred_scores shape = (625, 1, 171)\n",
      "Fold- 5 :  [1.7183433771133423, 0.8223999738693237]\n",
      "20/20 [==============================] - 1s 11ms/step\n",
      "pred_scores shape = (625, 1, 171)\n",
      "Fold- 6 :  [2.2258787155151367, 0.7247999906539917]\n",
      "20/20 [==============================] - 0s 11ms/step\n",
      "pred_scores shape = (625, 1, 171)\n",
      "Fold- 7 :  [1.7343553304672241, 0.8288000226020813]\n",
      "20/20 [==============================] - 0s 11ms/step\n",
      "pred_scores shape = (625, 1, 171)\n",
      "Fold- 8 :  [1.5747936964035034, 0.8543999791145325]\n",
      "20/20 [==============================] - 0s 11ms/step\n",
      "pred_scores shape = (625, 1, 171)\n",
      "Fold- 9 :  [1.838038682937622, 0.8080000281333923]\n",
      "20/20 [==============================] - 0s 11ms/step\n",
      "pred_scores shape = (625, 1, 171)\n",
      "Fold- 10 :  [1.6883172988891602, 0.8333333134651184]\n",
      "20/20 [==============================] - 0s 10ms/step\n",
      "pred_scores shape = (624, 1, 171)\n",
      "Class Features = False -- 10-cross fold accuracy of Protein Fold Prediction of SCOPe using hmm is :0.8220533311367035\n",
      "\n",
      "# of Labels: 171\n",
      "# of Labels: 171\n",
      "hybrid_features count: (None, 1, 2512)\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'): # use GPU\n",
    "\tf=0 # fold number\n",
    "\tconfig=tf.compat.v1.ConfigProto()\n",
    "\tconfig.gpu_options.allow_growth = True\n",
    "\tconfig.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "\ttf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\t\t\n",
    "\n",
    "\tacc_k_fold = [] # store the accuracy of each fold\n",
    "\tkf = StratifiedKFold(n_splits=10 shuffle=True, random_state=42) # 10-fold cross validation\n",
    "\n",
    "\tfor train, test in kf.split(sequences, labels): # split the data into training and testing sets\n",
    "\t\tf=f+1 # fold number\n",
    "\t\tX_train, X_test = matrixdata[train], matrixdata[test] # get training and testing data\n",
    "\t\tY_train, Y_test = foldlabels[train], foldlabels[test] # get training and testing labels\n",
    "\t\tX_biGram0_Train, X_biGram0_Test = biGram0[train], biGram0[test] # get training and testing bigram features\n",
    "\t\tX_biGram1_Train, X_biGram1_Test = biGram1[train], biGram1[test] # get training and testing bigram features\n",
    "\t\tX_biGram2_Train, X_biGram2_Test = biGram2[train], biGram2[test] # get training and testing bigram features\n",
    "\t\tX_biGram3_Train, X_biGram3_Test = biGram3[train], biGram3[test] # get training and testing bigram features\n",
    "\t\tX_biGram4_Train, X_biGram4_Test = biGram4[train], biGram4[test] # get training and testing bigram features \n",
    "\t\tX_biGram5_Train, X_biGram5_Test = biGram5[train], biGram5[test] # get training and testing bigram features\n",
    "\n",
    "\t\tcnn_input = Input(shape=(seqlen,20), name='cnn_input') # input layer\n",
    "\t\tc_input = Reshape((seqlen,20,1))(cnn_input) # reshape the input\n",
    "\t\tc_output1 = Conv2D(no_filters1, (5,5),  activation='tanh', strides=5, padding='same')(c_input) # convolution layer\n",
    "\t\tm_output1 = MaxPooling2D((3,3), strides=3, padding='same')(c_output1) # max pooling layer\n",
    "\t\tf_input = Flatten()(m_output1) # flatten the output\n",
    "\t\tf_input = tf.expand_dims(f_input, axis=1) # expand the dimensions\n",
    "\t\tbigram_input0 = Input(shape=(X_biGram0_Train.shape[1], X_biGram0_Train.shape[2]), name='bigram_input0') # input layer for bigram features\n",
    "\t\tbigram_input1 = Input(shape=(X_biGram1_Train.shape[1], X_biGram1_Train.shape[2]), name='bigram_input1') # input layer for bigram features\n",
    "\t\tbigram_input2 = Input(shape=(X_biGram2_Train.shape[1], X_biGram2_Train.shape[2]), name='bigram_input2') # input layer for bigram features\n",
    "\t\tbigram_input3 = Input(shape=(X_biGram3_Train.shape[1], X_biGram3_Train.shape[2]), name='bigram_input3') # input layer for bigram features\n",
    "\t\tbigram_input4 = Input(shape=(X_biGram4_Train.shape[1], X_biGram4_Train.shape[2]), name='bigram_input4') # input layer for bigram features\n",
    "\t\tbigram_input5 = Input(shape=(X_biGram5_Train.shape[1], X_biGram5_Train.shape[2]), name='bigram_input5') # input layer for bigram features\n",
    "\t\tbigram_input5 = Input(shape=(X_biGram5_Train.shape[1], X_biGram5_Train.shape[2]), name='bigram_input5') # input layer for bigram features\n",
    "        # concatenate the input layers\n",
    "\t\thybrid_features = concatenate([f_input, bigram_input0, bigram_input1, bigram_input2, bigram_input3, bigram_input4, bigram_input5], axis=2)\n",
    "\n",
    "\t\ty_train_shape = [Y_train.shape[0], 1, Y_train.shape[1]] # reshape the labels\n",
    "\t\ty_test_shape = [Y_test.shape[0], 1, Y_test.shape[1]] # reshape the labels\n",
    "\t\tY_train = tf.reshape(Y_train, y_train_shape) # reshape the labels\n",
    "\t\tY_test = tf.reshape(Y_test, y_test_shape) # reshape the labels\n",
    "\n",
    "\t\tl_output1 = layers.MultiHeadAttention(num_heads=2, key_dim=2) # multi-head attention layer\n",
    "\t\tattention_out1 = l_output1(hybrid_features, hybrid_features) # multi-head attention layer\n",
    "\t\tl_output1 = layers.Add()([attention_out1, hybrid_features]) # add the output of the attention layer to the input\n",
    "\t\tl_output1 = layers.LayerNormalization(epsilon=1e-6)(l_output1) # layer normalization\n",
    "\t\td_output1 = Dense(512, activation='relu')(l_output1) # dense layer\n",
    "\t\td_output1 = Dense(2512, activation='relu')(d_output1) # dense layer\n",
    "\n",
    "\t\td_output_last = Dense(512, activation='tanh')(d_output1) # dense layer\n",
    "\t\td_output_last = Dense(128, activation='tanh')(d_output_last) # dense layer\n",
    "\t\t# output layer\n",
    "\t\tmain_output = Dense(foldlabels.shape[1], activation='softmax', name='main_output', kernel_regularizer=l2(0.01))(d_output_last)\n",
    "\t\t# create the model\n",
    "\t\tmodel = Model(inputs=[cnn_input, bigram_input0, bigram_input1, bigram_input2, bigram_input3, bigram_input4, bigram_input5], outputs=[main_output])\n",
    "\t\tmodel.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy']) # compile the model\n",
    "\n",
    "\t\tearlyStopping = EarlyStopping(monitor='val_accuracy', patience=50, verbose=0, mode='auto') # early stopper\n",
    "\t\tload_file = \"./model/\"+dataset+\"_SXG_BiGram_best.h5\" # model saving path\n",
    "\t\tcheckpointer = ModelCheckpoint(monitor='val_accuracy', filepath=load_file, verbose=0, save_best_only=True) # checkpointer\n",
    "\n",
    "\t\thistory=model.fit({'cnn_input': X_train, 'bigram_input0': X_biGram0_Train, 'bigram_input1': X_biGram1_Train, 'bigram_input2': X_biGram2_Train, 'bigram_input3': X_biGram3_Train, 'bigram_input4': X_biGram4_Train, 'bigram_input5': X_biGram5_Train}, {'main_output': Y_train}, \n",
    "\t\t\tvalidation_data=({'cnn_input': X_test, 'bigram_input0': X_biGram0_Test, 'bigram_input1': X_biGram1_Test, 'bigram_input2': X_biGram2_Test, 'bigram_input3': X_biGram3_Test, 'bigram_input4': X_biGram4_Test, 'bigram_input5': X_biGram5_Test},{'main_output': Y_test}), \n",
    "\t\t\tepochs=500, batch_size=64, callbacks=[checkpointer, earlyStopping], verbose=0) # train the model\n",
    "\n",
    "\t\tmodel.load_weights(load_file) # load the best model\n",
    "\t\t# score using the best model\n",
    "\t\tscore = model.evaluate({'cnn_input': X_test, 'bigram_input0': X_biGram0_Test, 'bigram_input1': X_biGram1_Test, 'bigram_input2': X_biGram2_Test, 'bigram_input3': X_biGram3_Test, 'bigram_input4': X_biGram4_Test, 'bigram_input5': X_biGram5_Test},{'main_output': Y_test}, verbose=0, batch_size=1)\n",
    "\t\tprint(\"Fold-\",f, \": \", score) # print score\n",
    "\n",
    "\t\tacc_k_fold.append(score[1]) # append accuracy to the list\n",
    "\n",
    "\t\t# predict the scores\n",
    "\t\tpred_scores = model.predict({'cnn_input': X_test, 'bigram_input0': X_biGram0_Test, 'bigram_input1': X_biGram1_Test, 'bigram_input2': X_biGram2_Test, 'bigram_input3': X_biGram3_Test, 'bigram_input4': X_biGram4_Test, 'bigram_input5': X_biGram5_Test})\n",
    "\t\tprint(\"pred_scores shape =\", pred_scores.shape)\n",
    "\n",
    "\n",
    "\tresdata = \"Class Features = \"+str(clsfeature)+\" -- 10-cross fold accuracy of Protein \"+predtype+\" Prediction of \"+dataset+\" using \"+rawdata+\" is :\"+str(np.mean(acc_k_fold))+\"\\n\"\n",
    "\n",
    "\tprint(resdata)\n",
    "\tprint(\"10 Fold Accuracies:\", acc_k_fold)\n",
    "\n",
    "print(\"# of Labels:\", num_classes)\n",
    "print(\"# of Labels:\", foldlabels.shape[1])\n",
    "print(\"hybrid_features count:\", hybrid_features.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fold- 1 :  [1.1856778860092163, 0.8428571224212646]\n",
    "3/3 [==============================] - 1s 33ms/step\n",
    "pred_scores shape = (70, 1, 27)\n",
    "Fold- 2 :  [0.7193385362625122, 0.8714285492897034]\n",
    "3/3 [==============================] - 1s 40ms/step\n",
    "pred_scores shape = (70, 1, 27)\n",
    "Fold- 3 :  [0.7449996471405029, 0.9142857193946838]\n",
    "3/3 [==============================] - 1s 32ms/step\n",
    "pred_scores shape = (70, 1, 27)\n",
    "Fold- 4 :  [1.0007456541061401, 0.8714285492897034]\n",
    "3/3 [==============================] - 1s 25ms/step\n",
    "pred_scores shape = (70, 1, 27)\n",
    "Fold- 5 :  [1.1810381412506104, 0.8571428656578064]\n",
    "3/3 [==============================] - 1s 43ms/step\n",
    "pred_scores shape = (70, 1, 27)\n",
    "Fold- 6 :  [0.9316855072975159, 0.8695651888847351]\n",
    "3/3 [==============================] - 1s 28ms/step\n",
    "pred_scores shape = (69, 1, 27)\n",
    "Fold- 7 :  [1.3379615545272827, 0.8405796885490417]\n",
    "3/3 [==============================] - 1s 23ms/step\n",
    "pred_scores shape = (69, 1, 27)\n",
    "Fold- 8 :  [0.9299620389938354, 0.8840579986572266]\n",
    "3/3 [==============================] - 1s 33ms/step\n",
    "pred_scores shape = (69, 1, 27)\n",
    "Fold- 9 :  [1.2655516862869263, 0.8550724387168884]\n",
    "3/3 [==============================] - 1s 35ms/step\n",
    "pred_scores shape = (69, 1, 27)\n",
    "Fold- 10 :  [0.6408186554908752, 0.8985507488250732]\n",
    "3/3 [==============================] - 1s 26ms/step\n",
    "pred_scores shape = (69, 1, 27)\n",
    "Class Features = False -- 10-cross fold accuracy of Protein Fold Prediction of dd using hmm is :0.8704968869686127\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1mGBcn0gnXY0h4gLPuBRNIn-pnujeC7uc",
     "timestamp": 1668444601340
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
