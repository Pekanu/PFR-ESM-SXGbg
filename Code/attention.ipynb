{"cells":[{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":5975,"status":"ok","timestamp":1668444681644,"user":{"displayName":"AKASH SINGH IIIT Dharwad","userId":"09449469462329399013"},"user_tz":-330},"id":"11y_109IaNyM"},"outputs":[],"source":["import glob\n","import numpy as np\n","import pandas as pd\n","import csv\n","import h5py\n","import tensorflow as tf\n","import keras\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import Layer, Dense, Dropout, LSTM, GRU, Conv1D, Conv2D, Conv2DTranspose, MaxPooling2D, AveragePooling2D, UpSampling2D\n","from tensorflow.keras.layers import concatenate, GlobalMaxPooling1D, Flatten, BatchNormalization\n","from tensorflow.keras.layers import Activation, Reshape, TimeDistributed, Embedding, Input\n","from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adamax, Adadelta, Adagrad, Nadam\n","from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.regularizers import l1, l2\n","from keras import backend as K\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from tqdm import tqdm\n","\n","import parse_files as p\n","from features import bigram_features0, bigram_features1, bigram_features2, bigram_features3, bigram_features4, bigram_features5\n","\n","from numpy.random import seed\n","from tensorflow.python.keras.backend import set_session\n","from keras import regularizers"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"XcsZAKFl-uy0"},"outputs":[],"source":["# class TransformerBlock(layers.Layer):\n","#   def __init__(self, embed_dim, num_heads, foldlabels, learning_rate=0.001):\n","#     super(TransformerBlock, self).__init__()\n","\n","#     self.attention = layers.MultiHeadAttention(num_heads, embed_dim)\n","#     self.dropout1 = layers.Dropout(learning_rate)\n","#     self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n","\n","#     self.feed_forward_network = keras.Sequential([\n","#         Dense(512, activation='tanh'),\n","# \t\t    Dense(128, activation='tanh')\n","#     ])\n","\n","#     self.droput2 = layers.Dropout(learning_rate)\n","#     self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n","#     self.softmax = Dense(foldlabels.shape[1], activation='softmax', name='main_output', kernel_regularizer=l2(0.01))\n","\n","#   def call(self, inputs):\n","#     batch_size, seq_len = tf.shape(inputs)[0], tf.shape(inputs)[1]"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# clsfeature = True\n","clsfeature = False\n","\n","\n","rawdata = 'hmm'\n","# rawdata = 'pssm'\n","\n","# predtype = 'Class'\n","predtype = 'Fold'\n","\n","# dataset = 'dd'\n","# dataset = 'edd'\n","# dataset = 'tg'\n","dataset = 'SCOPe'\n","#dataset = '25_SCOPe_DDEDDTG'"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# if dataset == 'dd':\n","#     sgt_df = pd.read_csv('../Datasets/dd.csv')\n","# elif dataset == 'edd':\n","#     sgt_df = pd.read_csv('../Datasets/edd.csv')\n","# else:\n","#     sgt_df = pd.read_csv('../Datasets/tg.csv')\n","\n","# sgt_df['Sequence'] = sgt_df['Sequence'].str.lower()\n","# sgt_df.head()\n","\n","# A = []\n","# for i in sgt_df['Sequence']:\n","#   a = (\" \".join(i))\n","#   A.append(a)\n","# sgt_df['Sequence'] = A\n","# sgt_df['Sequence'].iloc[0]\n","\n","# sgt_df['Sequence'] = sgt_df['Sequence'].str.split(' ')\n","\n","# sgt = SGT(kappa = 10, flatten=True)\n","# all_sequences = []\n","# useless_sequences = []\n","# for sequence in tqdm(range(len(sgt_df['Sequence']))):\n","#     try:\n","#         encoded_sequence = sgt.fit(sgt_df['Sequence'].iloc[sequence])\n","#         all_sequences.append(encoded_sequence)\n","#     except ValueError:\n","#         print('Useless Sequence')\n","#         useless_sequences.append(sequence)\n","\n","# all_sequences = np.array(all_sequences)\n","# all_sequences.shape\n","\n","# all_sequences = np.reshape(all_sequences, (all_sequences.shape[0], 1, all_sequences.shape[1]))\n","# all_sequences.shape"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["np.random.seed(0)\n","tf.random.set_seed(0)\n","\n","labels = []\n","hmm = []\n","pssm = []\n","seq = []\n","seqlen = 200\n","biGram_features0 = []\n","biGram_features1 = []\n","biGram_features2 = []\n","biGram_features3 = []\n","biGram_features4 = []\n","biGram_features5 = []\n","\n","\n","# Load all the filenames of PSSM's\n","filelist = glob.glob('./data/'+dataset+'/'+rawdata+'/*.txt')\n","\n","# Read all the labels of the given dataset\n","if dataset == \"SCOPe\":\n","\tlabel_for_seq = pd.read_csv(\"./astral_2_08_final.csv\") # Make sure all the sequences are in uppercase\n","else:\n","\tlabel_for_seq = p.load_labels('./data/'+dataset+'_'+predtype+'_labels.txt')\n","\n","# Read all the HMM and PSSM matrices of the given dataset\n","for i in range(0, len(filelist)):\n","\t# HMM data\n","\tif(rawdata == 'hmm'):\n","\t\tseq_hmm,prob_hmm,extras_hmm = p.parse_hmm(filelist[i])\n","\t\ttempseq = seq_hmm.upper()\n","\t\tseq.append(tempseq)\n","\t\tif dataset == \"SCOPe\":\n","\t\t\tlabels.append(label_for_seq.loc[label_for_seq[\"sequence\"] == tempseq][\"fold\"].values[0])\n","\t\telse:\n","\t\t\tlabels.append(label_for_seq[seq_hmm.upper()])\n","\t\tif(clsfeature):\n","\t\t\tbiGram_features0.append(((np.append((bigram_features0(prob_hmm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n","\t\t\tbiGram_features1.append(((np.append((bigram_features1(prob_hmm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n","\t\t\tbiGram_features2.append(((np.append((bigram_features2(prob_hmm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n","\t\t\tbiGram_features3.append(((np.append((bigram_features3(prob_hmm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n","\t\t\tbiGram_features4.append(((np.append((bigram_features4(prob_hmm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n","\t\t\tbiGram_features5.append(((np.append((bigram_features5(prob_hmm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n","\t\telse:\n","\t\t\tbiGram_features0.append(bigram_features0(prob_hmm))\n","\t\t\tbiGram_features2.append(bigram_features2(prob_hmm))\n","\t\t\tbiGram_features3.append(bigram_features3(prob_hmm))\n","\t\t\tbiGram_features4.append(bigram_features4(prob_hmm))\n","\t\t\tbiGram_features1.append(bigram_features1(prob_hmm))\n","\t\t\tbiGram_features5.append(bigram_features5(prob_hmm))\n","\n","\t\tnorm_hmm = prob_hmm + 0.01\n","\t\tif(len(norm_hmm) < seqlen):\n","\t\t\tfor j in range(seqlen-len(norm_hmm)):\n","\t\t\t\tnorm_hmm = np.concatenate((norm_hmm,norm_hmm[0]*0))\n","\t\telse:\n","\t\t\tnorm_hmm = norm_hmm[:seqlen]\n","\t\thmm.append(norm_hmm)\n","\n","\t# PSSM data\n","\telse:  \n","\t\tseq_pssm,prob_pssm,lprob_pssm,extra_pssm = p.parse_pssm(filelist[i])\n","\t\ttempseq = seq_pssm.upper()\n","\t\tseq.append(tempseq)\n","\t\tlabels.append(label_for_seq[seq_pssm.upper()])\t    \n","\t\tif(clsfeature):\n","\t\t\tbiGram_features0.append(((np.append((bigram_features0(prob_pssm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n","\t\t\tbiGram_features1.append(((np.append((bigram_features1(prob_pssm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n","\t\t\tbiGram_features2.append(((np.append((bigram_features2(prob_pssm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n","\t\t\tbiGram_features3.append(((np.append((bigram_features3(prob_pssm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n","\t\t\tbiGram_features4.append(((np.append((bigram_features4(prob_pssm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n","\t\t\tbiGram_features5.append(((np.append((bigram_features5(prob_pssm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n","\t\telse:\n","\t\t\tbiGram_features0.append(bigram_features0(prob_pssm))\n","\t\t\tbiGram_features2.append(bigram_features2(prob_pssm))\n","\t\t\tbiGram_features3.append(bigram_features3(prob_pssm))\n","\t\t\tbiGram_features4.append(bigram_features4(prob_pssm))\n","\t\t\tbiGram_features1.append(bigram_features1(prob_pssm))\n","\t\t\tbiGram_features5.append(bigram_features5(prob_pssm))\n","\n","\t\tnorm_pssm = prob_pssm + 0.01\n","\n","\t\tif(len(norm_pssm) < seqlen):\n","\t\t\tfor j in range(seqlen-len(norm_pssm)):\n","\t\t\t\tnorm_pssm = np.concatenate((norm_pssm,norm_pssm[0]*0))\n","\t\telse:\n","\t\t\tnorm_pssm = norm_pssm[:seqlen]\n","\t\tpssm.append(norm_pssm)\n","\n","labels = np.array(labels)\n","# print(\"Labels=\",labels)\n","num_classes =  len(np.unique(labels))\n","foldlabels = pd.get_dummies(labels).values\n","# print(\"foldlabels=\",foldlabels)\n","sequences = np.array(seq)\n","# print(sequences.shape)\n","biGram0 = np.array(biGram_features0)\n","biGram1 = np.array(biGram_features1)\n","biGram2 = np.array(biGram_features2)\n","biGram3 = np.array(biGram_features3)\n","biGram4 = np.array(biGram_features4)\n","biGram5 = np.array(biGram_features5)\n","# np.random.shuffle(hmm)\n","# np.random.shuffle(pssm)\n","hmm = np.array(hmm)\n","pssm = np.array(pssm)\n","\n","if(rawdata == 'hmm'):\n","\tmatrixdata = hmm\n","else:\n","\tmatrixdata = pssm\n","\n","no_filters1 = 4"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"vLUYxzRZbVKA"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_6728\\1542452974.py:6: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n","\n","Fold- 1 :  [1.637765645980835, 0.8464000225067139]\n","20/20 [==============================] - 1s 11ms/step\n","pred_scores shape = (625, 1, 171)\n","Fold- 2 :  [1.6080728769302368, 0.8464000225067139]\n","20/20 [==============================] - 0s 11ms/step\n","pred_scores shape = (625, 1, 171)\n","Fold- 3 :  [1.7202656269073486, 0.8399999737739563]\n","20/20 [==============================] - 0s 11ms/step\n","pred_scores shape = (625, 1, 171)\n","Fold- 4 :  [1.7978019714355469, 0.8159999847412109]\n","20/20 [==============================] - 0s 11ms/step\n","pred_scores shape = (625, 1, 171)\n","Fold- 5 :  [1.7183433771133423, 0.8223999738693237]\n","20/20 [==============================] - 1s 11ms/step\n","pred_scores shape = (625, 1, 171)\n","Fold- 6 :  [2.2258787155151367, 0.7247999906539917]\n","20/20 [==============================] - 0s 11ms/step\n","pred_scores shape = (625, 1, 171)\n","Fold- 7 :  [1.7343553304672241, 0.8288000226020813]\n","20/20 [==============================] - 0s 11ms/step\n","pred_scores shape = (625, 1, 171)\n","Fold- 8 :  [1.5747936964035034, 0.8543999791145325]\n","20/20 [==============================] - 0s 11ms/step\n","pred_scores shape = (625, 1, 171)\n","Fold- 9 :  [1.838038682937622, 0.8080000281333923]\n","20/20 [==============================] - 0s 11ms/step\n","pred_scores shape = (625, 1, 171)\n","Fold- 10 :  [1.6883172988891602, 0.8333333134651184]\n","20/20 [==============================] - 0s 10ms/step\n","pred_scores shape = (624, 1, 171)\n","Class Features = False -- 10-cross fold accuracy of Protein Fold Prediction of SCOPe using hmm is :0.8220533311367035\n","\n","# of Labels: 171\n","# of Labels: 171\n","hybrid_features count: (None, 1, 2512)\n"]}],"source":["with tf.device('/device:GPU:0'):\n","\tf=0\n","\tconfig=tf.compat.v1.ConfigProto()\n","\tconfig.gpu_options.allow_growth = True\n","\tconfig.gpu_options.per_process_gpu_memory_fraction = 0.2\n","\ttf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\t\t\n","\n","\tacc_k_fold = []\n","\t# kf = KFold(n_splits=10, shuffle=True, random_state=8)\n","\tkf = StratifiedKFold(n_splits=10 shuffle=True, random_state=42)\n","\n","\tfor train, test in kf.split(sequences, labels):\n","\t\tf=f+1\n","\t\tX_train, X_test = matrixdata[train], matrixdata[test]\n","\t\tY_train, Y_test = foldlabels[train], foldlabels[test]\n","\t\tX_biGram0_Train, X_biGram0_Test = biGram0[train], biGram0[test]\n","\t\tX_biGram1_Train, X_biGram1_Test = biGram1[train], biGram1[test]\n","\t\tX_biGram2_Train, X_biGram2_Test = biGram2[train], biGram2[test]\n","\t\tX_biGram3_Train, X_biGram3_Test = biGram3[train], biGram3[test]\n","\t\tX_biGram4_Train, X_biGram4_Test = biGram4[train], biGram4[test]\n","\t\tX_biGram5_Train, X_biGram5_Test = biGram5[train], biGram5[test]\n","\t\t# X_Sgt_Train, X_Sgt_Test = all_sequences[train], all_sequences[test]\n","\n","\t\t# print(\"X_train:\", X_train.shape)\n","\t\t# print(\"Y_train:\", Y_train.shape)\n","\t\t# print(\"X_biGram0_Train:\", X_biGram0_Train.shape)\n","\t\t# print(\"X_biGram1_Train:\", X_biGram1_Train.shape)\n","\t\t# print(\"X_biGram2_Train:\", X_biGram2_Train.shape)\n","\t\t# print(\"X_biGram3_Train:\", X_biGram3_Train.shape)\n","\t\t# print(\"X_biGram4_Train:\", X_biGram4_Train.shape)\n","\t\t# print(\"X_biGram5_Train:\", X_biGram5_Train.shape)\n","\n","\t\tcnn_input = Input(shape=(seqlen,20), name='cnn_input')\t\t\n","\t\tc_input = Reshape((seqlen,20,1))(cnn_input)\t\t\n","\t\tc_output1 = Conv2D(no_filters1, (5,5),  activation='tanh', strides=5, padding='same')(c_input)\n","\t\tm_output1 = MaxPooling2D((3,3), strides=3, padding='same')(c_output1)\n","\t\tf_input = Flatten()(m_output1)\n","\t\tf_input = tf.expand_dims(f_input, axis=1)\n","\t\t# print(\"Convolution Features Size: \", f_input.shape)\n","\t\tbigram_input0 = Input(shape=(X_biGram0_Train.shape[1], X_biGram0_Train.shape[2]), name='bigram_input0')\n","\t\t# bg0_input = Flatten()(bigram_input0)\n","\n","\t\tbigram_input1 = Input(shape=(X_biGram1_Train.shape[1], X_biGram1_Train.shape[2]), name='bigram_input1')\n","\t\t# bg1_input = Flatten()(bigram_input1)\n","\n","\t\tbigram_input2 = Input(shape=(X_biGram2_Train.shape[1], X_biGram2_Train.shape[2]), name='bigram_input2')\n","\t\t# bg2_input = Flatten()(bigram_input2)\n","\n","\t\tbigram_input3 = Input(shape=(X_biGram3_Train.shape[1], X_biGram3_Train.shape[2]), name='bigram_input3')\n","\t\t# bg3_input = Flatten()(bigram_input3)\n","\n","\t\tbigram_input4 = Input(shape=(X_biGram4_Train.shape[1], X_biGram4_Train.shape[2]), name='bigram_input4')\n","\t\t# bg4_input = Flatten()(bigram_input4)\n","\n","\t\tbigram_input5 = Input(shape=(X_biGram5_Train.shape[1], X_biGram5_Train.shape[2]), name='bigram_input5')\n","\t\t# bg5_input = Flatten()(bigram_input5)\n","\n","\t\tbigram_input5 = Input(shape=(X_biGram5_Train.shape[1], X_biGram5_Train.shape[2]), name='bigram_input5')\n","\t\t# bg5_input = Flatten()(bigram_input5)\n","\n","\t\t# sgt_input = Input(shape=(X_Sgt_Train.shape[1], X_Sgt_Train.shape[2]), name='sgt_input')\n","\t\t# sgt_input_flatten = Flatten()(sgt_input)\n","\n","\t\t# hybrid_features = concatenate([bg0_input, bg1_input, f_input, bg2_input, bg3_input, bg4_input, bg5_input, sgt_input_flatten], axis=-1)\n","\t\t# hybrid_features = bg0_input\n","\t\t# hybrid_features = concatenate([bg0_input, bg1_input, bg2_input, bg3_input, bg4_input, bg5_input], axis=-1)\n","\t\t# hybrid_features = concatenate([bigram_input0, bigram_input1, bigram_input2, bigram_input3, bigram_input4, bigram_input5], axis=2)\n","\t\t# hybrid_features = bigram_input0\n","\t\thybrid_features = concatenate([f_input, bigram_input0, bigram_input1, bigram_input2, bigram_input3, bigram_input4, bigram_input5], axis=2)\n","\n","\n","\t\ty_train_shape = [Y_train.shape[0], 1, Y_train.shape[1]]\n","\t\ty_test_shape = [Y_test.shape[0], 1, Y_test.shape[1]]\n","\t\tY_train = tf.reshape(Y_train, y_train_shape)\n","\t\tY_test = tf.reshape(Y_test, y_test_shape)\n","\n","\t\t# print(\"Hybrid Features Size: \", hybrid_features.shape)\n","\t\tl_output1 = layers.MultiHeadAttention(num_heads=2, key_dim=2)\n","\t\tattention_out1 = l_output1(hybrid_features, hybrid_features)\n","\t\tl_output1 = layers.Add()([attention_out1, hybrid_features])\n","\t\tl_output1 = layers.LayerNormalization(epsilon=1e-6)(l_output1)\n","\t\td_output1 = Dense(512, activation='relu')(l_output1)\n","\t\td_output1 = Dense(2512, activation='relu')(d_output1)\n","\t\t# l_input2 = layers.Add()([d_output1, l_output1])\n","\t\t# l_input2 = layers.LayerNormalization(epsilon=1e-6)(l_input2)\n","\t\t# l_output2 = layers.MultiHeadAttention(num_heads=2, key_dim=2)\n","\t\t# attention_out2 = l_output2(l_input2, l_input2)\n","\t\t# l_output2 = layers.Add()([attention_out2, l_input2])\n","\t\t# l_output2 = layers.LayerNormalization(epsilon=1e-6)(l_output2)\n","\t\t# d_output2 = Dense(512, activation='tanh')(l_output2)\n","\t\t# d_output2 = Dense(2512, activation='tanh')(d_output2)\n","\t\t# l_input3 = layers.Add()([d_output2, l_output2])\n","\t\t# l_input3 = layers.LayerNormalization(epsilon=1e-6)(l_input3)\n","\t\t# l_output3 = layers.MultiHeadAttention(num_heads=2, key_dim=2)\n","\t\t# attention_out3 = l_output3(l_input3, l_input3)\n","\t\t# l_output3 = layers.Add()([attention_out3, l_input3])\n","\t\t# l_output3 = layers.LayerNormalization(epsilon=1e-6)(l_output3)\n","\t\t# d_output3 = Dense(512, activation='tanh')(l_output3)\n","\t\t# d_output3 = Dense(2512, activation='tanh')(d_output2)\n","\t\t# l_input4 = layers.Add()([d_output3, l_output3])\n","\t\t# l_input4 = layers.LayerNormalization(epsilon=1e-6)(l_input3)\n","\t\t# l_output4 = layers.MultiHeadAttention(num_heads=2, key_dim=2)\n","\t\t# attention_out4 = l_output4(l_input4, l_input4)\n","\t\t# l_output4 = layers.Add()([attention_out4, l_input4])\n","\t\t# l_output4 = layers.LayerNormalization(epsilon=1e-6)(l_output4)\n","\t\t# d_output4 = Dense(512, activation='tanh')(l_output4)\n","\t\t# d_output4 = Dense(2512, activation='tanh')(d_output4)\n","\t\t# l_input5 = layers.Add()([l_output4, d_output4])\n","\t\t# l_input5 = layers.LayerNormalization(epsilon=1e-6)(l_input5)\n","\t\t# l_output5 = layers.MultiHeadAttention(num_heads=2, key_dim=2)\n","\t\t# attention_out5 = l_output5(l_input5, l_input5)\n","\t\t# l_output5 = layers.Add()([attention_out5, l_input5])\n","\t\t# l_output5 = layers.LayerNormalization(epsilon=1e-6)(l_output5)\n","\t\t# d_output5 = Dense(512, activation='tanh')(l_output5)\n","\t\t# d_output5 = Dense(2512, activation='tanh')(d_output5)\n","\t\t# l_input6 = layers.Add()([l_output5, d_output5])\n","\t\t# l_input6 = layers.LayerNormalization(epsilon=1e-6)(l_input6)\n","\t\t# l_output6 = layers.MultiHeadAttention(num_heads=2, key_dim=2)\n","\t\t# attention_out6 = l_output6(l_input6, l_input6)\n","\t\t# l_output6 = layers.Add()([attention_out6, l_input6])\n","\t\t# l_output6 = layers.LayerNormalization(epsilon=1e-6)(l_output6)\n","\t\t# d_output6 = Dense(512, activation='tanh')(l_output6)\n","\t\t# d_output6 = Dense(2512, activation='tanh')(d_output6)\n","\t\t# l_input7 = layers.Add()([l_output6, d_output6])\n","\t\t# l_input7 = layers.LayerNormalization(epsilon=1e-6)(l_input7)\n","\t\td_output_last = Dense(512, activation='tanh')(d_output1)\n","\t\td_output_last = Dense(128, activation='tanh')(d_output_last)\n","\t\tmain_output = Dense(foldlabels.shape[1], activation='softmax', name='main_output', kernel_regularizer=l2(0.01))(d_output_last)\n","\t\tmodel = Model(inputs=[cnn_input, bigram_input0, bigram_input1, bigram_input2, bigram_input3, bigram_input4, bigram_input5], outputs=[main_output])\n","\t\tmodel.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n","\t\t# model.summary()\n","\n","\t\tearlyStopping = EarlyStopping(monitor='val_accuracy', patience=50, verbose=0, mode='auto')\n","\t\tload_file = \"./model/\"+dataset+\"_SXG_BiGram_best.h5\"\n","\t\tcheckpointer = ModelCheckpoint(monitor='val_accuracy', filepath=load_file, verbose=0, save_best_only=True)\n","\n","\t\thistory=model.fit({'cnn_input': X_train, 'bigram_input0': X_biGram0_Train, 'bigram_input1': X_biGram1_Train, 'bigram_input2': X_biGram2_Train, 'bigram_input3': X_biGram3_Train, 'bigram_input4': X_biGram4_Train, 'bigram_input5': X_biGram5_Train}, {'main_output': Y_train}, \n","\t\t\tvalidation_data=({'cnn_input': X_test, 'bigram_input0': X_biGram0_Test, 'bigram_input1': X_biGram1_Test, 'bigram_input2': X_biGram2_Test, 'bigram_input3': X_biGram3_Test, 'bigram_input4': X_biGram4_Test, 'bigram_input5': X_biGram5_Test},{'main_output': Y_test}), \n","\t\t\tepochs=500, batch_size=64, callbacks=[checkpointer, earlyStopping], verbose=0)\n","\n","\t\tmodel.load_weights(load_file)\n","\t\tscore = model.evaluate({'cnn_input': X_test, 'bigram_input0': X_biGram0_Test, 'bigram_input1': X_biGram1_Test, 'bigram_input2': X_biGram2_Test, 'bigram_input3': X_biGram3_Test, 'bigram_input4': X_biGram4_Test, 'bigram_input5': X_biGram5_Test},{'main_output': Y_test}, verbose=0, batch_size=1)\n","\t\tprint(\"Fold-\",f, \": \", score)\n","\n","\t\tacc_k_fold.append(score[1])\n","\n","\t\tpred_scores = model.predict({'cnn_input': X_test, 'bigram_input0': X_biGram0_Test, 'bigram_input1': X_biGram1_Test, 'bigram_input2': X_biGram2_Test, 'bigram_input3': X_biGram3_Test, 'bigram_input4': X_biGram4_Test, 'bigram_input5': X_biGram5_Test})\n","\t\tprint(\"pred_scores shape =\", pred_scores.shape)\n","\n","\t\t# foldslabel=['Globin-like', 'Cytochrome C', 'DNA-binding 3-helical bundle', 'Immunoglobulin-like Beta-sandwich', 'Common fold of diphtheria toxin/trascription factors/transcription', 'Cupredoxins', 'TIM beta/alpha-barrel', 'NAD(P)-binding Rossmann-fold domains ', 'FAD/NAD(P)-binding domain', '4-Helical up-and-down bundle', '4-Helical cytokines', 'Aplha-EF-hand', 'SAM domain-like', 'Galactose-binding domain-like', 'Viral coat and capsid proteins', 'Concanavalin A-like lectins/glucanases', 'SH3-like barrel', 'OB-fold', 'Trefoil', 'Trypsin-like serine protease', 'Lipocalins', 'Double-stranded Beta-helix', 'Flavodoxin-like', 'Adenine nucleotide alpha hydrolase-like ', 'P-loop containing nucleoside triphosphate hydrolases ', 'Thioredoxin fold', 'Ribonuclease H-like motif', 'Phosphorylase/hydrolase-like', 'S-adenosyl-L-methionine-dependent methyltransferases', 'Alpha/beta-Hydrolases', 'Periplasmic binding protein-like I', 'Beta-Grasp (ubiquitin-like) ', 'Cystatin-like', 'Ferredoxin-like', 'Alpha-alpha super helix', 'Nucleoplasmin-like']\n","\n","\t\t# for i in range(pred_scores.shape[0]):\n","\t\t# \tif np.sum(pred_scores[i,:]) != 0:\n","\t\t# \t\tres=np.argmax(pred_scores[i,:])\n","\t\t# \t\tprint(\"Fold is:\", foldslabel[res])\n","\n","\t\t# \telse:\n","\t\t# \t\tprint(\"Danger\")\n","\n","\t\t# y_classes = y_prob.argmax(axis=-1)\n","\t\t# print(\"Model Evaluate output\", y_classes)\n","\n","\n","\tresdata = \"Class Features = \"+str(clsfeature)+\" -- 10-cross fold accuracy of Protein \"+predtype+\" Prediction of \"+dataset+\" using \"+rawdata+\" is :\"+str(np.mean(acc_k_fold))+\"\\n\"\n","\n","\tprint(resdata)\n","\tprint(\"10 Fold Accuracies:\", acc_k_fold)\n","\n","print(\"# of Labels:\", num_classes)\n","print(\"# of Labels:\", foldlabels.shape[1])\n","print(\"hybrid_features count:\", hybrid_features.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Fold- 1 :  [1.1856778860092163, 0.8428571224212646]\n","3/3 [==============================] - 1s 33ms/step\n","pred_scores shape = (70, 1, 27)\n","Fold- 2 :  [0.7193385362625122, 0.8714285492897034]\n","3/3 [==============================] - 1s 40ms/step\n","pred_scores shape = (70, 1, 27)\n","Fold- 3 :  [0.7449996471405029, 0.9142857193946838]\n","3/3 [==============================] - 1s 32ms/step\n","pred_scores shape = (70, 1, 27)\n","Fold- 4 :  [1.0007456541061401, 0.8714285492897034]\n","3/3 [==============================] - 1s 25ms/step\n","pred_scores shape = (70, 1, 27)\n","Fold- 5 :  [1.1810381412506104, 0.8571428656578064]\n","3/3 [==============================] - 1s 43ms/step\n","pred_scores shape = (70, 1, 27)\n","Fold- 6 :  [0.9316855072975159, 0.8695651888847351]\n","3/3 [==============================] - 1s 28ms/step\n","pred_scores shape = (69, 1, 27)\n","Fold- 7 :  [1.3379615545272827, 0.8405796885490417]\n","3/3 [==============================] - 1s 23ms/step\n","pred_scores shape = (69, 1, 27)\n","Fold- 8 :  [0.9299620389938354, 0.8840579986572266]\n","3/3 [==============================] - 1s 33ms/step\n","pred_scores shape = (69, 1, 27)\n","Fold- 9 :  [1.2655516862869263, 0.8550724387168884]\n","3/3 [==============================] - 1s 35ms/step\n","pred_scores shape = (69, 1, 27)\n","Fold- 10 :  [0.6408186554908752, 0.8985507488250732]\n","3/3 [==============================] - 1s 26ms/step\n","pred_scores shape = (69, 1, 27)\n","Class Features = False -- 10-cross fold accuracy of Protein Fold Prediction of dd using hmm is :0.8704968869686127\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1mGBcn0gnXY0h4gLPuBRNIn-pnujeC7uc","timestamp":1668444601340}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
