{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5975,
     "status": "ok",
     "timestamp": 1668444681644,
     "user": {
      "displayName": "AKASH SINGH IIIT Dharwad",
      "userId": "09449469462329399013"
     },
     "user_tz": -330
    },
    "id": "11y_109IaNyM"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This script contains code for SXGbgFold model\n",
    "'''\n",
    "# import libraries\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Layer, Dense, Dropout, LSTM, GRU, Conv1D, Conv2D, Conv2DTranspose, MaxPooling2D, AveragePooling2D, UpSampling2D\n",
    "from tensorflow.keras.layers import concatenate, GlobalMaxPooling1D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Activation, Reshape, TimeDistributed, Embedding, Input\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adamax, Adadelta, Adagrad, Nadam\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "import parse_files as p\n",
    "from features import bigram_features0, bigram_features1, bigram_features2, bigram_features3, bigram_features4, bigram_features5\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsfeature = False # True/False (True if you want to use class features. Use only when predtype is Fold)\n",
    "rawdata = 'hmm' # hmm/pssm\n",
    "predtype = 'Fold' # Class/Fold\n",
    "dataset = 'SCOPe' # dd/edd/tg/SCOPe/25_SCOPe_DDEDDTG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 420\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# initialize variables to store data and labels\n",
    "labels = []\n",
    "hmm = []\n",
    "pssm = []\n",
    "seq = []\n",
    "seqlen = 200\n",
    "biGram_features0 = []\n",
    "biGram_features1 = []\n",
    "biGram_features2 = []\n",
    "biGram_features3 = []\n",
    "biGram_features4 = []\n",
    "biGram_features5 = []\n",
    "\n",
    "\n",
    "# Load all the filenames of PSSM's\n",
    "filelist = glob.glob('./data/'+dataset+'/'+rawdata+'/*.txt')\n",
    "\n",
    "# Read all the labels of the given dataset\n",
    "if dataset == \"SCOPe\":\n",
    "\tlabel_for_seq = pd.read_csv(\"./astral_2_08_final.csv\") # Make sure all the sequences are in uppercase\n",
    "else:\n",
    "\tlabel_for_seq = p.load_labels('./data/'+dataset+'_'+predtype+'_labels.txt')\n",
    "\n",
    "# Read all the HMM and PSSM matrices of the given dataset\n",
    "for i in range(0, len(filelist)):\n",
    "\t# HMM data\n",
    "\tif(rawdata == 'hmm'): # HMM data\n",
    "\t\tseq_hmm,prob_hmm,extras_hmm = p.parse_hmm(filelist[i]) # Parse HMM file\n",
    "\t\ttempseq = seq_hmm.upper() # Convert sequence to uppercase\n",
    "\t\tseq.append(tempseq) # Append sequence to seq list\n",
    "\t\t# Append label to labels list\n",
    "\t\tif dataset == \"SCOPe\":\n",
    "\t\t\tlabels.append(label_for_seq.loc[label_for_seq[\"sequence\"] == tempseq][\"fold\"].values[0])\n",
    "\t\telse:\n",
    "\t\t\tlabels.append(label_for_seq[seq_hmm.upper()])\n",
    "\t\tif(clsfeature): # If class features are to be used\n",
    "\t\t\t# Append bigram features and class label to biGram_features list\n",
    "\t\t\tbiGram_features0.append(((np.append((bigram_features0(prob_hmm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\t\tbiGram_features1.append(((np.append((bigram_features1(prob_hmm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\t\tbiGram_features2.append(((np.append((bigram_features2(prob_hmm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\t\tbiGram_features3.append(((np.append((bigram_features3(prob_hmm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\t\tbiGram_features4.append(((np.append((bigram_features4(prob_hmm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\t\tbiGram_features5.append(((np.append((bigram_features5(prob_hmm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\telse: # If class features are not to be used\n",
    "\t\t\t# Append bigram features to biGram_features list\n",
    "\t\t\tbiGram_features0.append(bigram_features0(prob_hmm))\n",
    "\t\t\tbiGram_features2.append(bigram_features2(prob_hmm))\n",
    "\t\t\tbiGram_features3.append(bigram_features3(prob_hmm))\n",
    "\t\t\tbiGram_features4.append(bigram_features4(prob_hmm))\n",
    "\t\t\tbiGram_features1.append(bigram_features1(prob_hmm))\n",
    "\t\t\tbiGram_features5.append(bigram_features5(prob_hmm))\n",
    "\n",
    "\t\tnorm_hmm = prob_hmm + 0.01\n",
    "\t\tif(len(norm_hmm) < seqlen): # If length of HMM data is less than seqlen, pad it with zeros\n",
    "\t\t\tfor j in range(seqlen-len(norm_hmm)):\n",
    "\t\t\t\tnorm_hmm = np.concatenate((norm_hmm,norm_hmm[0]*0))\n",
    "\t\telse: # If length of HMM data is greater than seqlen, truncate it\n",
    "\t\t\tnorm_hmm = norm_hmm[:seqlen]\n",
    "\t\thmm.append(norm_hmm) # Append HMM data to hmm list\n",
    "\n",
    "\t# PSSM data\n",
    "\telse:  \n",
    "\t\tseq_pssm,prob_pssm,lprob_pssm,extra_pssm = p.parse_pssm(filelist[i]) # Parse PSSM file\n",
    "\t\ttempseq = seq_pssm.upper() # Convert sequence to uppercase\n",
    "\t\tseq.append(tempseq) # Append sequence to seq list\n",
    "\t\tlabels.append(label_for_seq[seq_pssm.upper()]) # Append label to labels list\n",
    "\t\tif(clsfeature): # If class features are to be used\n",
    "\t\t\tbiGram_features0.append(((np.append((bigram_features0(prob_pssm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\t\tbiGram_features1.append(((np.append((bigram_features1(prob_pssm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\t\tbiGram_features2.append(((np.append((bigram_features2(prob_pssm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\t\tbiGram_features3.append(((np.append((bigram_features3(prob_pssm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\t\tbiGram_features4.append(((np.append((bigram_features4(prob_pssm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\t\tbiGram_features5.append(((np.append((bigram_features5(prob_pssm)), (p.get_class_label(dataset, label_for_seq[tempseq])))).reshape(1, -1)))\n",
    "\t\telse: # If class features are not to be used\n",
    "\t\t\tbiGram_features0.append(bigram_features0(prob_pssm))\n",
    "\t\t\tbiGram_features2.append(bigram_features2(prob_pssm))\n",
    "\t\t\tbiGram_features3.append(bigram_features3(prob_pssm))\n",
    "\t\t\tbiGram_features4.append(bigram_features4(prob_pssm))\n",
    "\t\t\tbiGram_features1.append(bigram_features1(prob_pssm))\n",
    "\t\t\tbiGram_features5.append(bigram_features5(prob_pssm))\n",
    "\n",
    "\t\tnorm_pssm = prob_pssm + 0.01\n",
    "\n",
    "\t\tif(len(norm_pssm) < seqlen): # If length of PSSM data is less than seqlen, pad it with zeros\n",
    "\t\t\tfor j in range(seqlen-len(norm_pssm)): # Pad the data with zeros\n",
    "\t\t\t\tnorm_pssm = np.concatenate((norm_pssm,norm_pssm[0]*0))\n",
    "\t\telse: # If length of PSSM data is greater than seqlen, truncate it\n",
    "\t\t\tnorm_pssm = norm_pssm[:seqlen]\n",
    "\t\tpssm.append(norm_pssm) # Append PSSM data to pssm list\n",
    "\n",
    "# Convert labels, sequences and bigram features to numpy arrays\n",
    "labels = np.array(labels)\n",
    "num_classes =  len(np.unique(labels))\n",
    "foldlabels = pd.get_dummies(labels).values\n",
    "sequences = np.array(seq)\n",
    "biGram0 = np.array(biGram_features0)\n",
    "biGram1 = np.array(biGram_features1)\n",
    "biGram2 = np.array(biGram_features2)\n",
    "biGram3 = np.array(biGram_features3)\n",
    "biGram4 = np.array(biGram_features4)\n",
    "biGram5 = np.array(biGram_features5)\n",
    "hmm = np.array(hmm)\n",
    "pssm = np.array(pssm)\n",
    "\n",
    "if(rawdata == 'hmm'): # If rawdata is hmm, set matrixdata to hmm\n",
    "\tmatrixdata = hmm\n",
    "else: # If rawdata is pssm, set matrixdata to pssm\n",
    "\tmatrixdata = pssm\n",
    "\n",
    "no_filters1 = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLUYxzRZbVKA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_4572\\2728612245.py:6: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n",
      "Fold- 1 :  [1.6671605110168457, 0.8543999791145325]\n",
      "20/20 [==============================] - 0s 5ms/step\n",
      "pred_scores shape = (625, 171)\n",
      "Fold- 2 :  [1.7678273916244507, 0.8416000008583069]\n",
      "20/20 [==============================] - 0s 5ms/step\n",
      "pred_scores shape = (625, 171)\n",
      "Fold- 3 :  [1.8220089673995972, 0.8223999738693237]\n",
      "20/20 [==============================] - 0s 4ms/step\n",
      "pred_scores shape = (625, 171)\n",
      "Fold- 4 :  [1.8152962923049927, 0.8303999900817871]\n",
      "20/20 [==============================] - 0s 5ms/step\n",
      "pred_scores shape = (625, 171)\n",
      "Fold- 5 :  [1.8196617364883423, 0.8191999793052673]\n",
      "20/20 [==============================] - 0s 5ms/step\n",
      "pred_scores shape = (625, 171)\n",
      "Fold- 6 :  [1.718684196472168, 0.8399999737739563]\n",
      "20/20 [==============================] - 0s 5ms/step\n",
      "pred_scores shape = (625, 171)\n",
      "Fold- 7 :  [1.6944830417633057, 0.86080002784729]\n",
      "20/20 [==============================] - 0s 5ms/step\n",
      "pred_scores shape = (625, 171)\n",
      "Fold- 8 :  [1.675512671470642, 0.8543999791145325]\n",
      "20/20 [==============================] - 0s 5ms/step\n",
      "pred_scores shape = (625, 171)\n",
      "Fold- 9 :  [1.7694470882415771, 0.8256000280380249]\n",
      "20/20 [==============================] - 0s 4ms/step\n",
      "pred_scores shape = (625, 171)\n",
      "Fold- 10 :  [1.78073251247406, 0.8253205418586731]\n",
      "20/20 [==============================] - 0s 5ms/step\n",
      "pred_scores shape = (624, 171)\n",
      "Class Features = False -- 10-cross fold accuracy of Protein Fold Prediction of SCOPe using hmm is :0.8374120473861695\n",
      "\n",
      "10 Fold Accuracies: [0.8543999791145325, 0.8416000008583069, 0.8223999738693237, 0.8303999900817871, 0.8191999793052673, 0.8399999737739563, 0.86080002784729, 0.8543999791145325, 0.8256000280380249, 0.8253205418586731]\n",
      "# of Labels: 171\n",
      "# of Labels: 171\n",
      "hybrid_features count: (None, 2512)\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'): # Use GPU\n",
    "\tf=0 # Initialize fold count\n",
    "\tconfig=tf.compat.v1.ConfigProto()\n",
    "\tconfig.gpu_options.allow_growth = True\n",
    "\tconfig.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "\ttf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\t\t\n",
    "\n",
    "\tacc_k_fold = [] # Initialize list to store accuracies of each fold\n",
    "\tkf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) # Initialize 10-fold cross validation\n",
    "\n",
    "\tfor train, test in kf.split(sequences, labels): # Loop through each fold\n",
    "\t\tf=f+1 # Increment fold count\n",
    "\t\tX_train, X_test = matrixdata[train], matrixdata[test] # Split matrixdata into training and testing data\n",
    "\t\tY_train, Y_test = foldlabels[train], foldlabels[test] # Split foldlabels into training and testing labels\n",
    "\t\tX_biGram0_Train, X_biGram0_Test = biGram0[train], biGram0[test] # Split biGram0 into training and testing data\n",
    "\t\tX_biGram1_Train, X_biGram1_Test = biGram1[train], biGram1[test] # Split biGram1 into training and testing data\n",
    "\t\tX_biGram2_Train, X_biGram2_Test = biGram2[train], biGram2[test] # Split biGram2 into training and testing data\n",
    "\t\tX_biGram3_Train, X_biGram3_Test = biGram3[train], biGram3[test] # Split biGram3 into training and testing data\n",
    "\t\tX_biGram4_Train, X_biGram4_Test = biGram4[train], biGram4[test] # Split biGram4 into training and testing data\n",
    "\t\tX_biGram5_Train, X_biGram5_Test = biGram5[train], biGram5[test] # Split biGram5 into training and testing data\n",
    "\n",
    "\t\tcnn_input = Input(shape=(seqlen,20), name='cnn_input') # Initialize input layer for CNN\n",
    "\t\tc_input = Reshape((seqlen,20,1))(cnn_input) # Reshape input layer\n",
    "\t\tc_output1 = Conv2D(no_filters1, (5,5),  activation='tanh', strides=5, padding='same')(c_input) # Convolution layer\n",
    "\t\tm_output1 = MaxPooling2D((3,3), strides=3, padding='same')(c_output1) # Max pooling layer\n",
    "\t\tf_input = Flatten()(m_output1) # Flatten the output of max pooling layer\n",
    "\n",
    "\t\tbigram_input0 = Input(shape=(X_biGram0_Train.shape[1], X_biGram0_Train.shape[2]), name='bigram_input0') # Initialize input layer for bigram features\n",
    "\t\tbg0_input = Flatten()(bigram_input0) # Flatten the input layer\n",
    "\n",
    "\t\tbigram_input1 = Input(shape=(X_biGram1_Train.shape[1], X_biGram1_Train.shape[2]), name='bigram_input1') # Initialize input layer for bigram features\n",
    "\t\tbg1_input = Flatten()(bigram_input1) # Flatten the input layer\n",
    "\n",
    "\t\tbigram_input2 = Input(shape=(X_biGram2_Train.shape[1], X_biGram2_Train.shape[2]), name='bigram_input2') # Initialize input layer for bigram features\n",
    "\t\tbg2_input = Flatten()(bigram_input2) # Flatten the input layer\n",
    "\n",
    "\t\tbigram_input3 = Input(shape=(X_biGram3_Train.shape[1], X_biGram3_Train.shape[2]), name='bigram_input3') # Initialize input layer for bigram features\n",
    "\t\tbg3_input = Flatten()(bigram_input3) # Flatten the input layer\n",
    "\n",
    "\t\tbigram_input4 = Input(shape=(X_biGram4_Train.shape[1], X_biGram4_Train.shape[2]), name='bigram_input4') # Initialize input layer for bigram features\n",
    "\t\tbg4_input = Flatten()(bigram_input4) # Flatten the input layer\n",
    "\n",
    "\t\tbigram_input5 = Input(shape=(X_biGram5_Train.shape[1], X_biGram5_Train.shape[2]), name='bigram_input5') # Initialize input layer for bigram features\n",
    "\t\tbg5_input = Flatten()(bigram_input5) # Flatten the input layer\n",
    "\n",
    "\t\tbigram_input5 = Input(shape=(X_biGram5_Train.shape[1], X_biGram5_Train.shape[2]), name='bigram_input5') # Initialize input layer for bigram features\n",
    "\t\tbg5_input = Flatten()(bigram_input5) # Flatten the input layer\n",
    "\n",
    "\t\thybrid_features = concatenate([f_input, bg0_input, bg1_input, bg2_input, bg3_input, bg4_input, bg5_input], axis=-1) # Concatenate the features\n",
    "\t\td_output2 = Dense(512, activation='tanh')(hybrid_features) # Dense layer\n",
    "\t\td_output2 = Dense(128, activation='tanh')(d_output2) # Dense layer\n",
    "\t\tmain_output = Dense(foldlabels.shape[1], activation='softmax', name='main_output', kernel_regularizer=l2(0.01))(d_output2) # Output layer\n",
    "\n",
    "\t\t# Create model\n",
    "\t\tmodel = Model(inputs=[cnn_input, bigram_input0, bigram_input1, bigram_input2, bigram_input3, bigram_input4, bigram_input5], outputs=[main_output])\n",
    "\t\tmodel.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\t\tearlyStopping = EarlyStopping(monitor='val_accuracy', patience=50, verbose=0, mode='auto') # early stopper\n",
    "\t\tload_file = \"./model/\"+dataset+\"_SXG_BiGram_best.h5\" # model save file path\n",
    "\t\tcheckpointer = ModelCheckpoint(monitor='val_accuracy', filepath=load_file, verbose=0, save_best_only=True) # checkpointer\n",
    "\n",
    "\t\t# Fit the model\n",
    "\t\thistory=model.fit({'cnn_input': X_train, 'bigram_input0': X_biGram0_Train, 'bigram_input1': X_biGram1_Train, 'bigram_input2': X_biGram2_Train, 'bigram_input3': X_biGram3_Train, 'bigram_input4': X_biGram4_Train, 'bigram_input5': X_biGram5_Train}, {'main_output': Y_train}, \n",
    "\t\t\tvalidation_data=({'cnn_input': X_test, 'bigram_input0': X_biGram0_Test, 'bigram_input1': X_biGram1_Test, 'bigram_input2': X_biGram2_Test, 'bigram_input3': X_biGram3_Test, 'bigram_input4': X_biGram4_Test, 'bigram_input5': X_biGram5_Test},{'main_output': Y_test}), \n",
    "\t\t\tepochs=500, batch_size=64, callbacks=[checkpointer, earlyStopping], verbose=0)\n",
    "\n",
    "\t\tmodel.load_weights(load_file) # Load the best model and score\n",
    "\t\tscore = model.evaluate({'cnn_input': X_test, 'bigram_input0': X_biGram0_Test, 'bigram_input1': X_biGram1_Test, 'bigram_input2': X_biGram2_Test, 'bigram_input3': X_biGram3_Test, 'bigram_input4': X_biGram4_Test, 'bigram_input5': X_biGram5_Test},{'main_output': Y_test}, verbose=0, batch_size=1)\n",
    "\t\tprint(\"Fold-\",f, \": \", score)\n",
    "\n",
    "\t\tacc_k_fold.append(score[1]) # Append accuracy of the fold to acc_k_fold list\n",
    "\n",
    "\t\t# Predict the scores\n",
    "\t\tpred_scores = model.predict({'cnn_input': X_test, 'bigram_input0': X_biGram0_Test, 'bigram_input1': X_biGram1_Test, 'bigram_input2': X_biGram2_Test, 'bigram_input3': X_biGram3_Test, 'bigram_input4': X_biGram4_Test, 'bigram_input5': X_biGram5_Test})\n",
    "\t\tprint(\"pred_scores shape =\", pred_scores.shape)\n",
    "\n",
    "\n",
    "\tresdata = \"Class Features = \"+str(clsfeature)+\" -- 10-cross fold accuracy of Protein \"+predtype+\" Prediction of \"+dataset+\" using \"+rawdata+\" is :\"+str(np.mean(acc_k_fold))+\"\\n\"\n",
    "\n",
    "\tprint(resdata) # Print the result\n",
    "\tprint(\"10 Fold Accuracies:\", acc_k_fold) # Print the accuracies of each fold\n",
    "\n",
    "print(\"# of Labels:\", num_classes)\n",
    "print(\"# of Labels:\", foldlabels.shape[1])\n",
    "print(\"hybrid_features count:\", hybrid_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1mGBcn0gnXY0h4gLPuBRNIn-pnujeC7uc",
     "timestamp": 1668444601340
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.10.5 ('protein-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f81477a04ac803d382ae7d6eef639787df1946e019eec2800176f10d07e3db8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
